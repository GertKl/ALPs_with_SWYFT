{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7885c4b0-ccb2-49d9-b298-9fd9b7a9c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import swyft\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import importlib\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "device_notebook = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import wandb\n",
    "import copy\n",
    "from torch.multiprocessing import Pool\n",
    "torch.multiprocessing.set_start_method('spawn',force=True)\n",
    "torch.set_num_threads(28)\n",
    "import itertools\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "sys.path.append('/home/gertwk/ALPs_with_SWYFT/analysis_scripts/ALP_sim')\n",
    "from explim_functions import generate_expected_limits\n",
    "import sympy as sy\n",
    "from scipy.stats import norm, lognorm\n",
    "from swyft.plot.mass import _get_jefferys_interval as interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56feaf19-6395-4d74-a1ff-6d089c02f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"ALPs_with_SWYFT\"\n",
    "thesis_figs = os.getcwd().split(main_dir)[0]+\"/\"+main_dir+\"/thesis_figures/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "875729e6-31ab-4306-bc54-618e6d81cd3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/gertwk/ALPs_with_SWYFT/cluster_runs/analysis_results/grid_informed_power3/truncation_record.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(priors[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_phys\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file: config_objects \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m config_objects\u001b[38;5;241m.\u001b[39mkeys(): priors[name][key] \u001b[38;5;241m=\u001b[39m config_objects[key]\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(priors[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation_record\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file: config_objects \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m config_objects\u001b[38;5;241m.\u001b[39mkeys(): priors[name][key] \u001b[38;5;241m=\u001b[39m config_objects[key]\n\u001b[1;32m     38\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremove(priors[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/swyft4-dev-notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/gertwk/ALPs_with_SWYFT/cluster_runs/analysis_results/grid_informed_power3/truncation_record.pickle'"
     ]
    }
   ],
   "source": [
    "names = ['grid_informed_power3',]\n",
    "colors_priors = ['r','#FFA500','y','g','b', ]\n",
    "\n",
    "priors = {}\n",
    "for ip, name in enumerate(names):\n",
    "\n",
    "    priors[name] = {'name': name}\n",
    "\n",
    "    priors[name]['results_path'] = '/home/gertwk/ALPs_with_SWYFT/cluster_runs/analysis_results/'+name\n",
    "\n",
    "    priors[name]['store_path'] = priors[name]['results_path']+\"/sim_output/store\"\n",
    "\n",
    "    priors[name]['config_vars'] = priors[name]['results_path'] +'/config_variables.pickle'\n",
    "\n",
    "    priors[name]['config_phys'] = priors[name]['results_path'] +'/physics_variables.pickle'\n",
    "    \n",
    "    priors[name]['truncation_record'] = priors[name]['results_path'] +'/truncation_record.pickle'\n",
    "\n",
    "    removed_ALP_sim=0\n",
    "    try:\n",
    "        sys.path.remove('/home/gertwk/ALPs_with_SWYFT/analysis_scripts/ALP_sim')\n",
    "        removed_ALP_sim=1\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        del sys.modules['ALP_quick_sim']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    sys.path.append(priors[name]['results_path'])\n",
    "    import param_function\n",
    "    import ALP_quick_sim\n",
    "    with open(priors[name]['config_vars'], 'rb') as file: config_objects = pickle.load(file)\n",
    "    for key in config_objects.keys(): priors[name][key] = config_objects[key]\n",
    "    with open(priors[name]['config_phys'], 'rb') as file: config_objects = pickle.load(file)\n",
    "    for key in config_objects.keys(): priors[name][key] = config_objects[key]\n",
    "    with open(priors[name]['truncation_record'], 'rb') as file: config_objects = pickle.load(file)\n",
    "    for key in config_objects.keys(): priors[name][key] = config_objects[key]\n",
    "    sys.path.remove(priors[name]['results_path'])\n",
    "    sys.path.append(priors[name]['results_path']+'/train_output/net')\n",
    "    import network\n",
    "    sys.path.remove(priors[name]['results_path']+'/train_output/net')\n",
    "    \n",
    "    count = 0\n",
    "    for combo in itertools.product(*priors[name]['hyperparams'].values()):\n",
    "        if count == priors[name]['which_grid_point']:\n",
    "            hyperparams_point = {}\n",
    "            for i, key in enumerate(priors[name]['hyperparams'].keys()):\n",
    "                hyperparams_point[key]=combo[i]\n",
    "        count +=1\n",
    "\n",
    "    priors[name]['net_path'] = {}\n",
    "    priors[name]['net'] = {}\n",
    "    for rnd in range(priors[name]['which_truncation']+1):\n",
    "        round = 'round_'+str(rnd)\n",
    "        priors[name]['net_path'][round] = (priors[name]['results_path'] + '/train_output/net/trained_network_'\n",
    "                                                         +round+'_gridpoint_'+str(priors[name]['which_grid_point'])+'.pt')\n",
    "        priors[name]['net'][round] = network.NetworkCorner(\n",
    "            nbins=priors[name]['A'].nbins,\n",
    "            marginals=priors[name]['POI_indices'],\n",
    "            param_names=priors[name]['A'].param_names,\n",
    "            **hyperparams_point,\n",
    "        )\n",
    "        priors[name]['net'][round].load_state_dict(torch.load(priors[name]['net_path'][round]))\n",
    "\n",
    "    with open(priors[name]['results_path']+'/explim_predictions.pickle', 'rb') as file:\n",
    "        priors[name]['predictions'] = pickle.load(file)\n",
    "\n",
    "    if priors[name]['which_truncation'] > 0:\n",
    "        store = swyft.ZarrStore(priors[name]['store_path'] + \"/\" + priors[name]['store_name']+\"_round_\"+str(priors[name]['which_truncation'])+\"_gridpoint_\"+str(priors[name]['which_grid_point']))\n",
    "        store_explim = swyft.ZarrStore(priors[name]['store_path'] + \"/\" + priors[name]['store_name']+\"_explim_round_\"+str(priors[name]['which_truncation'])+\"_gridpoint_\"+str(priors[name]['which_grid_point']))\n",
    "        store_prior = swyft.ZarrStore(priors[name]['store_path'] + \"/\" + priors[name]['store_name']+\"_prior_round_\"+str(priors[name]['which_truncation'])+\"_gridpoint_\"+str(priors[name]['which_grid_point']))\n",
    "    else:\n",
    "        store = swyft.ZarrStore(priors[name]['store_path'] + \"/\" + priors[name]['store_name'])\n",
    "        store_explim = swyft.ZarrStore(priors[name]['store_path'] + \"/\" + priors[name]['store_name']+\"_explim\")\n",
    "        store_prior = swyft.ZarrStore(priors[name]['store_path'] + \"/\" + priors[name]['store_name']+\"_prior\")\n",
    "    priors[name]['samples'] = store.get_sample_store()\n",
    "    priors[name]['samples_explim'] = store_explim.get_sample_store()\n",
    "    priors[name]['samples_prior'] = store_prior.get_sample_store()\n",
    " \n",
    "    del sys.modules['param_function']\n",
    "    del sys.modules['ALP_quick_sim']\n",
    "    del sys.modules['network']\n",
    "    if removed_ALP_sim: sys.path.append('/home/gertwk/ALPs_with_SWYFT/analysis_scripts/ALP_sim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b4fe7-b2bd-4a4c-be5c-01cafebeb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = swyft.SwyftTrainer(accelerator = 'cuda', precision = 64,logger=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27e054-6bab-4c12-8d90-1374bf989d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/gertwk/ALPs_with_SWYFT/analysis_scripts/ALP_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af9786-3a1b-4247-a172-23c26ce443ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pair_to_index(pair,n_indices):\n",
    "    pair = sorted(pair)\n",
    "    return int((pair[0]+1)*(n_indices-1+n_indices-pair[0]-1)/2 - n_indices + pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d358e-2538-4bdf-883a-2da94cb5165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(exp,n_bins):\n",
    "    x = np.linspace(-1,1,n_bins)\n",
    "    return 0.5+0.5*np.cos(np.pi*np.sign(x)*np.abs(x)**exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e313d-fa3c-4992-ba22-386886c2e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdnorm = norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bf5e6-da8b-4da0-8a1f-47ba879fb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def references2D(\n",
    "    parsed_samples,\n",
    "    x_freq = None,\n",
    "    y_freq = None,\n",
    "    x_exponent = None,\n",
    "    y_exponent = None,\n",
    "    x_phase = None,\n",
    "    y_phase = None,\n",
    "    min_freq = 1,\n",
    "    max_freq = 1.4,\n",
    "    min_freq_gap = 0.15,\n",
    "    min_exponent = 0.8,\n",
    "    max_exponent = 1.5,\n",
    "    min_exponent_gap = 0.3,\n",
    "):\n",
    "  \n",
    "    try:\n",
    "        samples = parsed_samples['data']\n",
    "    except IndexError:\n",
    "        samples = parsed_samples\n",
    "\n",
    "    data_length = len(samples[0]) \n",
    "    start_of_range1 = np.random.randint(0,data_length)\n",
    "    end_of_range1 = (start_of_range1 + int(data_length/2)) #np.random.randint( start_of_range1+int(data_length/3) , start_of_range1+int(2*data_length/3) )\n",
    "    exp1 = np.random.uniform(0.5,2)\n",
    "    exp2 = np.random.uniform(0.5,2)\n",
    "    range1 = np.arange(start_of_range1,end_of_range1)%data_length\n",
    "    range2 = np.arange(end_of_range1,end_of_range1+(start_of_range1-end_of_range1)%data_length)%data_length\n",
    "\n",
    "    reordered_range1 = (np.concatenate([range1,range2])-int(data_length/2))%data_length\n",
    "    reordered_range2 = (np.concatenate([range2,range1])-int(data_length/2))%data_length\n",
    "    weights1 = weight(exp1,data_length)[reordered_range1]\n",
    "    weights2 = weight(exp2,data_length)[reordered_range2]\n",
    "\n",
    "    samples1 = samples.copy()\n",
    "    samples2 = samples.copy()\n",
    "    \n",
    "    # samples1 = samples[:,range1]\n",
    "    # samples2 = samples[:,range2]\n",
    " \n",
    "    bin_mins1 = np.min(samples1, axis=0)\n",
    "    bin_maxes1 = np.max(samples1, axis=0)\n",
    "    sums_of_standardized_bins1 = np.sum( weights1*((samples1-bin_mins1)/(bin_maxes1-bin_mins1)) ,axis=1)\n",
    "    sums_of_standardized_bins1 = np.where(np.isinf(sums_of_standardized_bins1),0,sums_of_standardized_bins1)\n",
    "    max_sum1 = np.max(sums_of_standardized_bins1)\n",
    "    min_sum1 = np.min(sums_of_standardized_bins1)\n",
    "    standardized_sums1 = (sums_of_standardized_bins1-min_sum1)/(max_sum1-min_sum1)\n",
    "\n",
    "    bin_mins2 = np.min(samples2, axis=0)\n",
    "    bin_maxes2 = np.max(samples2, axis=0)\n",
    "    sums_of_standardized_bins2 = np.sum( weights2*((samples2-bin_mins2)/(bin_maxes2-bin_mins2)) ,axis=1)\n",
    "    sums_of_standardized_bins2 = np.where(np.isinf(sums_of_standardized_bins2),0,sums_of_standardized_bins2)\n",
    "    max_sum2 = np.max(sums_of_standardized_bins2)\n",
    "    min_sum2 = np.min(sums_of_standardized_bins2)\n",
    "    standardized_sums2 = (sums_of_standardized_bins2-min_sum2)/(max_sum2-min_sum2)\n",
    "\n",
    "    sorted_sums1 = np.sort(standardized_sums1)\n",
    "    sorted_sums2 = np.sort(standardized_sums2)\n",
    "\n",
    "    fitted_parameters1 = lognorm.fit(standardized_sums1)\n",
    "    fitted_parameters2 = lognorm.fit(standardized_sums2)\n",
    "    \n",
    "    phase_function1 = lognorm(fitted_parameters1[0], fitted_parameters1[1], fitted_parameters1[2]).cdf\n",
    "    phase_function2 = lognorm(fitted_parameters2[0], fitted_parameters2[1], fitted_parameters2[2]).cdf\n",
    "\n",
    "    phase_for_check1 = phase_function1(sorted_sums1)\n",
    "    phase_for_check2 = phase_function2(sorted_sums2)\n",
    "    phase1 = phase_function1(standardized_sums1)\n",
    "    phase2 = phase_function2(standardized_sums2)\n",
    "\n",
    "    if x_phase is None: x_phase = np.random.uniform(0,1)\n",
    "    if y_phase is None: y_phase = np.random.uniform(0,1)\n",
    "\n",
    "    freq_range = max_freq-min_freq\n",
    "    if x_freq is None: x_freq = np.random.uniform(min_freq,max_freq)\n",
    "    if y_freq is None: y_freq = np.random.uniform(min(x_freq+min_freq_gap,max_freq)-min_freq, freq_range+max(x_freq-min_freq-min_freq_gap,0))%(freq_range) + min_freq\n",
    "    \n",
    "    if x_exponent is None: x_exponent = np.random.uniform(min_exponent,1) if x_freq > y_freq else np.random.uniform(1,max_exponent)\n",
    "    if y_exponent is None: y_exponent = np.random.uniform(min_exponent,min(1,x_exponent-min_exponent_gap)) if x_freq < y_freq else np.random.uniform(max(1,x_exponent+min_exponent_gap),max_exponent)\n",
    "\n",
    "    x_values_for_check = 0.5 + 0.5*np.sin(2*np.pi*(( x_freq*phase_for_check1)**x_exponent + x_phase ))\n",
    "    y_values_for_check = 0.5 + 0.5*np.sin(2*np.pi*(( y_freq*phase_for_check2)**y_exponent + y_phase ))\n",
    "    \n",
    "    x_values = 0.5 + 0.5*np.sin(2*np.pi*(( x_freq*phase1)**x_exponent + x_phase ))\n",
    "    y_values = 0.5 + 0.5*np.sin(2*np.pi*(( y_freq*phase2)**y_exponent + y_phase ))\n",
    "\n",
    "    random_variables = {\n",
    "        'range1':range1,\n",
    "        'range2':range2,\n",
    "        'x_freq':x_freq,\n",
    "        'y_freq':y_freq,\n",
    "        'x_exponent':x_exponent,\n",
    "        'y_exponent':y_exponent,\n",
    "        'x_phase':x_phase,\n",
    "        'y_phase':y_phase,\n",
    "        'sums1':standardized_sums1,\n",
    "        'sums2':standardized_sums2,\n",
    "        'sorted_sums1':sorted_sums1,\n",
    "        'sorted_sums2':sorted_sums2,\n",
    "        'fitted_parameters1': fitted_parameters1,\n",
    "        'fitted_parameters2': fitted_parameters2,\n",
    "    }\n",
    "    \n",
    "\n",
    "    return np.array([x_values,y_values]).transpose(), np.array([x_values_for_check,y_values_for_check]).transpose(), random_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024b750-691b-4a32-821b-07f633bf6123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d838032-6a6e-494d-9bf5-3a32aa253dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def references2D(\n",
    "    parsed_samples,\n",
    "    x_freq = None,\n",
    "    y_freq = None,\n",
    "    x_exponent = None,\n",
    "    y_exponent = None,\n",
    "    x_phase = None,\n",
    "    y_phase = None,\n",
    "    min_freq = 1,\n",
    "    max_freq = 1.4,\n",
    "    min_freq_gap = 0.15,\n",
    "    min_exponent = 0.8,\n",
    "    max_exponent = 1.5,\n",
    "    min_exponent_gap = 0.3,\n",
    "    device = 'cpu'\n",
    "):\n",
    "  \n",
    "    try:\n",
    "        samples = parsed_samples['data']\n",
    "    except IndexError:\n",
    "        samples = parsed_samples\n",
    "\n",
    "    data_length = len(samples[0]) \n",
    "    start_of_range1 = np.random.randint(0,data_length)\n",
    "    end_of_range1 = (start_of_range1 + int(data_length/2)) #np.random.randint( start_of_range1+int(data_length/3) , start_of_range1+int(2*data_length/3) )\n",
    "    exp1 = np.random.uniform(0.5,2)\n",
    "    exp2 = np.random.uniform(0.5,2)\n",
    "    range1 = np.arange(start_of_range1,end_of_range1)%data_length\n",
    "    range2 = np.arange(end_of_range1,end_of_range1+(start_of_range1-end_of_range1)%data_length)%data_length\n",
    "\n",
    "    reordered_range1 = (np.concatenate([range1,range2])-int(data_length/2))%data_length\n",
    "    reordered_range2 = (np.concatenate([range2,range1])-int(data_length/2))%data_length\n",
    "    weights1 = weight(exp1,data_length)[reordered_range1]\n",
    "    weights2 = weight(exp2,data_length)[reordered_range2]\n",
    "\n",
    "    samples1 = samples.copy()\n",
    "    samples2 = samples.copy()\n",
    "    \n",
    "    # samples1 = samples[:,range1]\n",
    "    # samples2 = samples[:,range2]\n",
    " \n",
    "    bin_mins1 = np.min(samples1, axis=0)\n",
    "    bin_maxes1 = np.max(samples1, axis=0)\n",
    "    sums_of_standardized_bins1 = np.sum( weights1*((samples1-bin_mins1)/(bin_maxes1-bin_mins1)) ,axis=1)\n",
    "    sums_of_standardized_bins1 = np.where(np.isinf(sums_of_standardized_bins1),0,sums_of_standardized_bins1)\n",
    "    max_sum1 = np.max(sums_of_standardized_bins1)\n",
    "    min_sum1 = np.min(sums_of_standardized_bins1)\n",
    "    standardized_sums1 = (sums_of_standardized_bins1-min_sum1)/(max_sum1-min_sum1)\n",
    "\n",
    "    bin_mins2 = np.min(samples2, axis=0)\n",
    "    bin_maxes2 = np.max(samples2, axis=0)\n",
    "    sums_of_standardized_bins2 = np.sum( weights2*((samples2-bin_mins2)/(bin_maxes2-bin_mins2)) ,axis=1)\n",
    "    sums_of_standardized_bins2 = np.where(np.isinf(sums_of_standardized_bins2),0,sums_of_standardized_bins2)\n",
    "    max_sum2 = np.max(sums_of_standardized_bins2)\n",
    "    min_sum2 = np.min(sums_of_standardized_bins2)\n",
    "    standardized_sums2 = (sums_of_standardized_bins2-min_sum2)/(max_sum2-min_sum2)\n",
    "\n",
    "    sorted_sums1 = np.sort(standardized_sums1)\n",
    "    sorted_sums2 = np.sort(standardized_sums2)\n",
    "\n",
    "    fitted_parameters1 = lognorm.fit(standardized_sums1)\n",
    "    fitted_parameters2 = lognorm.fit(standardized_sums2)\n",
    "    \n",
    "    phase_function1 = lognorm(fitted_parameters1[0], fitted_parameters1[1], fitted_parameters1[2]).cdf\n",
    "    phase_function2 = lognorm(fitted_parameters2[0], fitted_parameters2[1], fitted_parameters2[2]).cdf\n",
    "\n",
    "    phase_for_check1 = phase_function1(sorted_sums1)\n",
    "    phase_for_check2 = phase_function2(sorted_sums2)\n",
    "    phase1 = phase_function1(standardized_sums1)\n",
    "    phase2 = phase_function2(standardized_sums2)\n",
    "\n",
    "    if x_phase is None: x_phase = np.random.uniform(0,1)\n",
    "    if y_phase is None: y_phase = np.random.uniform(0,1)\n",
    "\n",
    "    freq_range = max_freq-min_freq\n",
    "    if x_freq is None: x_freq = np.random.uniform(min_freq,max_freq)\n",
    "    if y_freq is None: y_freq = np.random.uniform(min(x_freq+min_freq_gap,max_freq)-min_freq, freq_range+max(x_freq-min_freq-min_freq_gap,0))%(freq_range) + min_freq\n",
    "    \n",
    "    if x_exponent is None: x_exponent = np.random.uniform(min_exponent,1) if x_freq > y_freq else np.random.uniform(1,max_exponent)\n",
    "    if y_exponent is None: y_exponent = np.random.uniform(min_exponent,min(1,x_exponent-min_exponent_gap)) if x_freq < y_freq else np.random.uniform(max(1,x_exponent+min_exponent_gap),max_exponent)\n",
    "\n",
    "    x_values_for_check = 0.5 + 0.5*np.sin(2*np.pi*(( x_freq*phase_for_check1)**x_exponent + x_phase ))\n",
    "    y_values_for_check = 0.5 + 0.5*np.sin(2*np.pi*(( y_freq*phase_for_check2)**y_exponent + y_phase ))\n",
    "    \n",
    "    x_values = 0.5 + 0.5*np.sin(2*np.pi*(( x_freq*phase1)**x_exponent + x_phase ))\n",
    "    y_values = 0.5 + 0.5*np.sin(2*np.pi*(( y_freq*phase2)**y_exponent + y_phase ))\n",
    "\n",
    "    random_variables = {\n",
    "        'range1':range1,\n",
    "        'range2':range2,\n",
    "        'x_freq':x_freq,\n",
    "        'y_freq':y_freq,\n",
    "        'x_exponent':x_exponent,\n",
    "        'y_exponent':y_exponent,\n",
    "        'x_phase':x_phase,\n",
    "        'y_phase':y_phase,\n",
    "        'sums1':standardized_sums1,\n",
    "        'sums2':standardized_sums2,\n",
    "        'sorted_sums1':sorted_sums1,\n",
    "        'sorted_sums2':sorted_sums2,\n",
    "        'fitted_parameters1': fitted_parameters1,\n",
    "        'fitted_parameters2': fitted_parameters2,\n",
    "    }\n",
    "    \n",
    "\n",
    "    return np.array([x_values,y_values]).transpose(), np.array([x_values_for_check,y_values_for_check]).transpose(), random_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430502f-9e21-4f1c-a073-e11163d93372",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del sys.modules['DRP_test']\n",
    "except KeyError:\n",
    "    pass\n",
    "from DRP_test import get_drp_coverage, get_drp_coverage_torch, draw_DRP_samples_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc6b17a-8741-40da-9a08-73f03b2898d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc200e5-a768-457a-9983-26e706f7192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "name = names[0]\n",
    "n_samps = 1000\n",
    "n_prior_samps=1_000\n",
    "prior_samples = priors[name]['samples_prior'][:n_prior_samps]\n",
    "which_truncation = priors[name]['which_truncation']\n",
    "which_grid_point = priors[name]['which_grid_point']\n",
    "POIs = priors[name]['POI_indices']\n",
    "A = priors[name]['A']\n",
    "bounds = np.array(priors[name]['bounds_rounds'][which_grid_point][which_truncation])\n",
    "tries = 10\n",
    "n_sim = int(priors[name]['n_sim_train'][-1]/tries)\n",
    "\n",
    "\n",
    "for runi in range(tries):\n",
    "\n",
    "    samples = priors[name]['samples'][-n_samps:]\n",
    "    \n",
    "    draws1d = {}\n",
    "    draws2d = {}\n",
    "    weights1d = {}\n",
    "    weights2d = {}\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        draws1d[round],draws2d[round],weights1d[round],weights2d[round] = draw_DRP_samples_fast(\n",
    "            priors[name]['net'][round],\n",
    "            trainer,\n",
    "            samples,\n",
    "            prior_samples,\n",
    "        )\n",
    "    n_refs = 1000\n",
    "\n",
    "    print('Computing references...', flush=True, end=\"\")\n",
    "    references_2d = [\n",
    "        references2D(samples)[0] for _ in range(n_refs)\n",
    "    ]\n",
    "    print(' done.')\n",
    "\n",
    "\n",
    "    ecp_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    ecp_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    validation_sums = { 'round_'+str(rnd) : [{},{}] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    # for rnd in range(1):\n",
    "    #     round = 'round_'+str(rnd)\n",
    "    #     for ref_i in range(len(references_2d)):\n",
    "    \n",
    "    #         for i, key in enumerate(draws1d[round].keys()):\n",
    "            \n",
    "    #             ecp_pp[round][0][ref_i][key], alpha_pp[round][0][ref_i][key], ecp_zz[round][0][ref_i][key], alpha_zz[round][0][ref_i][key], f_pp[round][0][ref_i][key], f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "    #                 draws1d[round][key],\n",
    "    #                 samples['params'][:,[POIs[i]]],\n",
    "    #                 weights = weights1d[round][key],\n",
    "    #                 theta_names=A.param_names[POIs[i]],\n",
    "    #                 bounds = np.array(bounds)[[POIs[i]]],\n",
    "    #                 references = references_2d[ref_i],\n",
    "    #                 device='cuda'\n",
    "    #             )\n",
    "    \n",
    "    #             if ref_i == 0: validation_sums[round][0][key] = 0\n",
    "    #             uncertainty = (interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "    #             validation_sums[round][0][key] += np.sum(((ecp_pp[round][0][ref_i][key]-alpha_pp[round][0][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    \n",
    "    rows = len(POIs)\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        for ref_i in range(len(references_2d)): \n",
    "            row = 0\n",
    "            column = 0\n",
    "            for i, key in enumerate(draws2d[round].keys()):\n",
    "                row+=1\n",
    "                if row >= rows:\n",
    "                    column+=1\n",
    "                    row = 1+column    \n",
    "                ecp_pp[round][1][ref_i][key], alpha_pp[round][1][ref_i][key], ecp_zz[round][1][ref_i][key], alpha_zz[round][1][ref_i][key], f_pp[round][1][ref_i][key],f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "                    draws2d[round][key],\n",
    "                    samples['params'][:,[column,row]],\n",
    "                    weights = weights2d[round][key],\n",
    "                    theta_names=np.array(A.param_names)[[column,row]],\n",
    "                    bounds = np.array(bounds)[[column,row]],\n",
    "                    references = references_2d[ref_i],\n",
    "                    device='cuda'\n",
    "                )\n",
    "    \n",
    "                if ref_i == 0: validation_sums[round][1][key] = 0\n",
    "                uncertainty = (interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "                validation_sums[round][1][key] += np.sum(((ecp_pp[round][1][ref_i][key]-alpha_pp[round][1][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    # for i, key in enumerate(draws1d['round_0'].keys()):\n",
    "    #     print(key)\n",
    "    #     for rnd in range(which_truncation+1):\n",
    "    #         print(validation_sums['round_'+str(rnd)][0][key])\n",
    "    #     print()\n",
    "    \n",
    "    for i, key in enumerate(draws2d['round_3'].keys()):\n",
    "        print(key)\n",
    "        for rnd in [3]:\n",
    "            print(validation_sums['round_'+str(rnd)][1][key])\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0eb63b-7bc8-4861-80d5-397488389af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "runi = 0\n",
    "samples = priors[name]['samples'][-(runi+1)*n_samps-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c567dd-5471-4a72-90fb-f715251ad8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817befee-e007-4947-b42b-47541f825ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "name = names[0]\n",
    "n_samps = 1000\n",
    "n_prior_samps=1_000\n",
    "prior_samples = priors[name]['samples_prior'][:n_prior_samps]\n",
    "which_truncation = priors[name]['which_truncation']\n",
    "which_grid_point = priors[name]['which_grid_point']\n",
    "POIs = priors[name]['POI_indices']\n",
    "A = priors[name]['A']\n",
    "bounds = np.array(priors[name]['bounds_rounds'][which_grid_point][which_truncation])\n",
    "tries = 10\n",
    "n_sim = int(priors[name]['n_sim_train'][-1]/tries)\n",
    "\n",
    "\n",
    "for runi in range(tries):\n",
    "\n",
    "    samples = priors[name]['samples'][-(runi+1)*n_samps-1:-1]\n",
    "    \n",
    "    draws1d = {}\n",
    "    draws2d = {}\n",
    "    weights1d = {}\n",
    "    weights2d = {}\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        draws1d[round],draws2d[round],weights1d[round],weights2d[round] = draw_DRP_samples_fast(\n",
    "            priors[name]['net'][round],\n",
    "            trainer,\n",
    "            samples,\n",
    "            prior_samples,\n",
    "        )\n",
    "    n_refs = 1000\n",
    "\n",
    "    print('Computing references...', flush=True, end=\"\")\n",
    "    references_2d = [\n",
    "        references2D(samples)[0] for _ in range(n_refs)\n",
    "    ]\n",
    "    print(' done.')\n",
    "\n",
    "\n",
    "    ecp_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    ecp_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    validation_sums = { 'round_'+str(rnd) : [{},{}] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    # for rnd in range(1):\n",
    "    #     round = 'round_'+str(rnd)\n",
    "    #     for ref_i in range(len(references_2d)):\n",
    "    \n",
    "    #         for i, key in enumerate(draws1d[round].keys()):\n",
    "            \n",
    "    #             ecp_pp[round][0][ref_i][key], alpha_pp[round][0][ref_i][key], ecp_zz[round][0][ref_i][key], alpha_zz[round][0][ref_i][key], f_pp[round][0][ref_i][key], f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "    #                 draws1d[round][key],\n",
    "    #                 samples['params'][:,[POIs[i]]],\n",
    "    #                 weights = weights1d[round][key],\n",
    "    #                 theta_names=A.param_names[POIs[i]],\n",
    "    #                 bounds = np.array(bounds)[[POIs[i]]],\n",
    "    #                 references = references_2d[ref_i],\n",
    "    #                 device='cuda'\n",
    "    #             )\n",
    "    \n",
    "    #             if ref_i == 0: validation_sums[round][0][key] = 0\n",
    "    #             uncertainty = (interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "    #             validation_sums[round][0][key] += np.sum(((ecp_pp[round][0][ref_i][key]-alpha_pp[round][0][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    \n",
    "    rows = len(POIs)\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        for ref_i in range(len(references_2d)): \n",
    "            row = 0\n",
    "            column = 0\n",
    "            for i, key in enumerate(draws2d[round].keys()):\n",
    "                row+=1\n",
    "                if row >= rows:\n",
    "                    column+=1\n",
    "                    row = 1+column    \n",
    "                ecp_pp[round][1][ref_i][key], alpha_pp[round][1][ref_i][key], ecp_zz[round][1][ref_i][key], alpha_zz[round][1][ref_i][key], f_pp[round][1][ref_i][key],f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "                    draws2d[round][key],\n",
    "                    samples['params'][:,[column,row]],\n",
    "                    weights = weights2d[round][key],\n",
    "                    theta_names=np.array(A.param_names)[[column,row]],\n",
    "                    bounds = np.array(bounds)[[column,row]],\n",
    "                    references = references_2d[ref_i],\n",
    "                    device='cuda'\n",
    "                )\n",
    "    \n",
    "                if ref_i == 0: validation_sums[round][1][key] = 0\n",
    "                uncertainty = (interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "                validation_sums[round][1][key] += np.sum(((ecp_pp[round][1][ref_i][key]-alpha_pp[round][1][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    # for i, key in enumerate(draws1d['round_0'].keys()):\n",
    "    #     print(key)\n",
    "    #     for rnd in range(which_truncation+1):\n",
    "    #         print(validation_sums['round_'+str(rnd)][0][key])\n",
    "    #     print()\n",
    "    \n",
    "    for i, key in enumerate(draws2d['round_3'].keys()):\n",
    "        print(key)\n",
    "        for rnd in [3]:\n",
    "            print(validation_sums['round_'+str(rnd)][1][key])\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b900b58-5c03-452b-a19c-94802e2e723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "name = names[0]\n",
    "n_samps = 1000\n",
    "n_prior_samps=10_000\n",
    "prior_samples = priors[name]['samples_prior'][:n_prior_samps]\n",
    "which_truncation = priors[name]['which_truncation']\n",
    "which_grid_point = priors[name]['which_grid_point']\n",
    "POIs = priors[name]['POI_indices']\n",
    "A = priors[name]['A']\n",
    "bounds = np.array(priors[name]['bounds_rounds'][which_grid_point][which_truncation])\n",
    "tries = 10\n",
    "n_sim = int(priors[name]['n_sim_train'][-1]/tries)\n",
    "\n",
    "\n",
    "for runi in range(tries):\n",
    "\n",
    "    samples = priors[name]['samples'][-n_samps:]\n",
    "    \n",
    "    draws1d = {}\n",
    "    draws2d = {}\n",
    "    weights1d = {}\n",
    "    weights2d = {}\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        draws1d[round],draws2d[round],weights1d[round],weights2d[round] = draw_DRP_samples_fast(\n",
    "            priors[name]['net'][round],\n",
    "            trainer,\n",
    "            samples,\n",
    "            prior_samples,\n",
    "        )\n",
    "    n_refs = 1000\n",
    "\n",
    "    print('Computing references...', flush=True, end=\"\")\n",
    "    references_2d = [\n",
    "        references2D(samples)[0] for _ in range(n_refs)\n",
    "    ]\n",
    "    print(' done.')\n",
    "\n",
    "\n",
    "    ecp_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    ecp_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    validation_sums = { 'round_'+str(rnd) : [{},{}] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    # for rnd in range(1):\n",
    "    #     round = 'round_'+str(rnd)\n",
    "    #     for ref_i in range(len(references_2d)):\n",
    "    \n",
    "    #         for i, key in enumerate(draws1d[round].keys()):\n",
    "            \n",
    "    #             ecp_pp[round][0][ref_i][key], alpha_pp[round][0][ref_i][key], ecp_zz[round][0][ref_i][key], alpha_zz[round][0][ref_i][key], f_pp[round][0][ref_i][key], f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "    #                 draws1d[round][key],\n",
    "    #                 samples['params'][:,[POIs[i]]],\n",
    "    #                 weights = weights1d[round][key],\n",
    "    #                 theta_names=A.param_names[POIs[i]],\n",
    "    #                 bounds = np.array(bounds)[[POIs[i]]],\n",
    "    #                 references = references_2d[ref_i],\n",
    "    #                 device='cuda'\n",
    "    #             )\n",
    "    \n",
    "    #             if ref_i == 0: validation_sums[round][0][key] = 0\n",
    "    #             uncertainty = (interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "    #             validation_sums[round][0][key] += np.sum(((ecp_pp[round][0][ref_i][key]-alpha_pp[round][0][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    \n",
    "    rows = len(POIs)\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        for ref_i in range(len(references_2d)): \n",
    "            row = 0\n",
    "            column = 0\n",
    "            for i, key in enumerate(draws2d[round].keys()):\n",
    "                row+=1\n",
    "                if row >= rows:\n",
    "                    column+=1\n",
    "                    row = 1+column    \n",
    "                ecp_pp[round][1][ref_i][key], alpha_pp[round][1][ref_i][key], ecp_zz[round][1][ref_i][key], alpha_zz[round][1][ref_i][key], f_pp[round][1][ref_i][key],f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "                    draws2d[round][key],\n",
    "                    samples['params'][:,[column,row]],\n",
    "                    weights = weights2d[round][key],\n",
    "                    theta_names=np.array(A.param_names)[[column,row]],\n",
    "                    bounds = np.array(bounds)[[column,row]],\n",
    "                    references = references_2d[ref_i],\n",
    "                    device='cuda'\n",
    "                )\n",
    "    \n",
    "                if ref_i == 0: validation_sums[round][1][key] = 0\n",
    "                uncertainty = (interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "                validation_sums[round][1][key] += np.sum(((ecp_pp[round][1][ref_i][key]-alpha_pp[round][1][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    # for i, key in enumerate(draws1d['round_0'].keys()):\n",
    "    #     print(key)\n",
    "    #     for rnd in range(which_truncation+1):\n",
    "    #         print(validation_sums['round_'+str(rnd)][0][key])\n",
    "    #     print()\n",
    "    \n",
    "    for i, key in enumerate(draws2d['round_3'].keys()):\n",
    "        print(key)\n",
    "        for rnd in [3]:\n",
    "            print(validation_sums['round_'+str(rnd)][1][key])\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9fcac-fe81-4709-a95a-7df7588c3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "name = names[0]\n",
    "n_samps = 1000\n",
    "n_prior_samps=10_000\n",
    "prior_samples = priors[name]['samples_prior'][:n_prior_samps]\n",
    "which_truncation = priors[name]['which_truncation']\n",
    "which_grid_point = priors[name]['which_grid_point']\n",
    "POIs = priors[name]['POI_indices']\n",
    "A = priors[name]['A']\n",
    "bounds = np.array(priors[name]['bounds_rounds'][which_grid_point][which_truncation])\n",
    "tries = 10\n",
    "n_sim = int(priors[name]['n_sim_train'][-1]/tries)\n",
    "\n",
    "\n",
    "for runi in range(tries):\n",
    "\n",
    "    samples = priors[name]['samples'][-(runi+1)*n_samps-1:-1]\n",
    "    \n",
    "    draws1d = {}\n",
    "    draws2d = {}\n",
    "    weights1d = {}\n",
    "    weights2d = {}\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        draws1d[round],draws2d[round],weights1d[round],weights2d[round] = draw_DRP_samples_fast(\n",
    "            priors[name]['net'][round],\n",
    "            trainer,\n",
    "            samples,\n",
    "            prior_samples,\n",
    "        )\n",
    "    n_refs = 1000\n",
    "\n",
    "    print('Computing references...', flush=True, end=\"\")\n",
    "    references_2d = [\n",
    "        references2D(samples)[0] for _ in range(n_refs)\n",
    "    ]\n",
    "    print(' done.')\n",
    "\n",
    "\n",
    "    ecp_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    ecp_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    validation_sums = { 'round_'+str(rnd) : [{},{}] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    # for rnd in range(1):\n",
    "    #     round = 'round_'+str(rnd)\n",
    "    #     for ref_i in range(len(references_2d)):\n",
    "    \n",
    "    #         for i, key in enumerate(draws1d[round].keys()):\n",
    "            \n",
    "    #             ecp_pp[round][0][ref_i][key], alpha_pp[round][0][ref_i][key], ecp_zz[round][0][ref_i][key], alpha_zz[round][0][ref_i][key], f_pp[round][0][ref_i][key], f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "    #                 draws1d[round][key],\n",
    "    #                 samples['params'][:,[POIs[i]]],\n",
    "    #                 weights = weights1d[round][key],\n",
    "    #                 theta_names=A.param_names[POIs[i]],\n",
    "    #                 bounds = np.array(bounds)[[POIs[i]]],\n",
    "    #                 references = references_2d[ref_i],\n",
    "    #                 device='cuda'\n",
    "    #             )\n",
    "    \n",
    "    #             if ref_i == 0: validation_sums[round][0][key] = 0\n",
    "    #             uncertainty = (interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "    #             validation_sums[round][0][key] += np.sum(((ecp_pp[round][0][ref_i][key]-alpha_pp[round][0][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    \n",
    "    rows = len(POIs)\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        for ref_i in range(len(references_2d)): \n",
    "            row = 0\n",
    "            column = 0\n",
    "            for i, key in enumerate(draws2d[round].keys()):\n",
    "                row+=1\n",
    "                if row >= rows:\n",
    "                    column+=1\n",
    "                    row = 1+column    \n",
    "                ecp_pp[round][1][ref_i][key], alpha_pp[round][1][ref_i][key], ecp_zz[round][1][ref_i][key], alpha_zz[round][1][ref_i][key], f_pp[round][1][ref_i][key],f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "                    draws2d[round][key],\n",
    "                    samples['params'][:,[column,row]],\n",
    "                    weights = weights2d[round][key],\n",
    "                    theta_names=np.array(A.param_names)[[column,row]],\n",
    "                    bounds = np.array(bounds)[[column,row]],\n",
    "                    references = references_2d[ref_i],\n",
    "                    device='cuda'\n",
    "                )\n",
    "    \n",
    "                if ref_i == 0: validation_sums[round][1][key] = 0\n",
    "                uncertainty = (interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "                validation_sums[round][1][key] += np.sum(((ecp_pp[round][1][ref_i][key]-alpha_pp[round][1][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    # for i, key in enumerate(draws1d['round_0'].keys()):\n",
    "    #     print(key)\n",
    "    #     for rnd in range(which_truncation+1):\n",
    "    #         print(validation_sums['round_'+str(rnd)][0][key])\n",
    "    #     print()\n",
    "    \n",
    "    for i, key in enumerate(draws2d['round_3'].keys()):\n",
    "        print(key)\n",
    "        for rnd in [3]:\n",
    "            print(validation_sums['round_'+str(rnd)][1][key])\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81d9c4-ccf3-4887-a30e-123a72eab696",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "name = names[0]\n",
    "n_samps = 10_000\n",
    "n_prior_samps=1_000\n",
    "prior_samples = priors[name]['samples_prior'][:n_prior_samps]\n",
    "which_truncation = priors[name]['which_truncation']\n",
    "which_grid_point = priors[name]['which_grid_point']\n",
    "POIs = priors[name]['POI_indices']\n",
    "A = priors[name]['A']\n",
    "bounds = np.array(priors[name]['bounds_rounds'][which_grid_point][which_truncation])\n",
    "tries = 10\n",
    "n_sim = int(priors[name]['n_sim_train'][-1]/tries)\n",
    "\n",
    "\n",
    "for runi in range(tries):\n",
    "\n",
    "    samples = priors[name]['samples'][-n_samps:]\n",
    "    \n",
    "    draws1d = {}\n",
    "    draws2d = {}\n",
    "    weights1d = {}\n",
    "    weights2d = {}\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        draws1d[round],draws2d[round],weights1d[round],weights2d[round] = draw_DRP_samples_fast(\n",
    "            priors[name]['net'][round],\n",
    "            trainer,\n",
    "            samples,\n",
    "            prior_samples,\n",
    "        )\n",
    "    n_refs = 1000\n",
    "\n",
    "    print('Computing references...', flush=True, end=\"\")\n",
    "    references_2d = [\n",
    "        references2D(samples)[0] for _ in range(n_refs)\n",
    "    ]\n",
    "    print(' done.')\n",
    "\n",
    "\n",
    "    ecp_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    ecp_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    validation_sums = { 'round_'+str(rnd) : [{},{}] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    # for rnd in range(1):\n",
    "    #     round = 'round_'+str(rnd)\n",
    "    #     for ref_i in range(len(references_2d)):\n",
    "    \n",
    "    #         for i, key in enumerate(draws1d[round].keys()):\n",
    "            \n",
    "    #             ecp_pp[round][0][ref_i][key], alpha_pp[round][0][ref_i][key], ecp_zz[round][0][ref_i][key], alpha_zz[round][0][ref_i][key], f_pp[round][0][ref_i][key], f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "    #                 draws1d[round][key],\n",
    "    #                 samples['params'][:,[POIs[i]]],\n",
    "    #                 weights = weights1d[round][key],\n",
    "    #                 theta_names=A.param_names[POIs[i]],\n",
    "    #                 bounds = np.array(bounds)[[POIs[i]]],\n",
    "    #                 references = references_2d[ref_i],\n",
    "    #                 device='cuda'\n",
    "    #             )\n",
    "    \n",
    "    #             if ref_i == 0: validation_sums[round][0][key] = 0\n",
    "    #             uncertainty = (interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "    #             validation_sums[round][0][key] += np.sum(((ecp_pp[round][0][ref_i][key]-alpha_pp[round][0][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    \n",
    "    rows = len(POIs)\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        for ref_i in range(len(references_2d)): \n",
    "            row = 0\n",
    "            column = 0\n",
    "            for i, key in enumerate(draws2d[round].keys()):\n",
    "                row+=1\n",
    "                if row >= rows:\n",
    "                    column+=1\n",
    "                    row = 1+column    \n",
    "                ecp_pp[round][1][ref_i][key], alpha_pp[round][1][ref_i][key], ecp_zz[round][1][ref_i][key], alpha_zz[round][1][ref_i][key], f_pp[round][1][ref_i][key],f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "                    draws2d[round][key],\n",
    "                    samples['params'][:,[column,row]],\n",
    "                    weights = weights2d[round][key],\n",
    "                    theta_names=np.array(A.param_names)[[column,row]],\n",
    "                    bounds = np.array(bounds)[[column,row]],\n",
    "                    references = references_2d[ref_i],\n",
    "                    device='cuda'\n",
    "                )\n",
    "    \n",
    "                if ref_i == 0: validation_sums[round][1][key] = 0\n",
    "                uncertainty = (interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "                validation_sums[round][1][key] += np.sum(((ecp_pp[round][1][ref_i][key]-alpha_pp[round][1][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    # for i, key in enumerate(draws1d['round_0'].keys()):\n",
    "    #     print(key)\n",
    "    #     for rnd in range(which_truncation+1):\n",
    "    #         print(validation_sums['round_'+str(rnd)][0][key])\n",
    "    #     print()\n",
    "    \n",
    "    for i, key in enumerate(draws2d['round_3'].keys()):\n",
    "        print(key)\n",
    "        for rnd in [3]:\n",
    "            print(validation_sums['round_'+str(rnd)][1][key])\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bad9e-25a5-4f25-a5db-0cedb555d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "name = names[0]\n",
    "n_samps = 10_000\n",
    "n_prior_samps=10_000\n",
    "prior_samples = priors[name]['samples_prior'][:n_prior_samps]\n",
    "which_truncation = priors[name]['which_truncation']\n",
    "which_grid_point = priors[name]['which_grid_point']\n",
    "POIs = priors[name]['POI_indices']\n",
    "A = priors[name]['A']\n",
    "bounds = np.array(priors[name]['bounds_rounds'][which_grid_point][which_truncation])\n",
    "tries = 10\n",
    "n_sim = int(priors[name]['n_sim_train'][-1]/tries)\n",
    "\n",
    "\n",
    "for runi in range(tries):\n",
    "\n",
    "    samples = priors[name]['samples'][-n_samps:]\n",
    "    \n",
    "    draws1d = {}\n",
    "    draws2d = {}\n",
    "    weights1d = {}\n",
    "    weights2d = {}\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        draws1d[round],draws2d[round],weights1d[round],weights2d[round] = draw_DRP_samples_fast(\n",
    "            priors[name]['net'][round],\n",
    "            trainer,\n",
    "            samples,\n",
    "            prior_samples,\n",
    "        )\n",
    "    n_refs = 1000\n",
    "\n",
    "    print('Computing references...', flush=True, end=\"\")\n",
    "    references_2d = [\n",
    "        references2D(samples)[0] for _ in range(n_refs)\n",
    "    ]\n",
    "    print(' done.')\n",
    "\n",
    "\n",
    "    ecp_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    ecp_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    alpha_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    f_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    validation_sums = { 'round_'+str(rnd) : [{},{}] for rnd in range(which_truncation+1) }\n",
    "    \n",
    "    # for rnd in range(1):\n",
    "    #     round = 'round_'+str(rnd)\n",
    "    #     for ref_i in range(len(references_2d)):\n",
    "    \n",
    "    #         for i, key in enumerate(draws1d[round].keys()):\n",
    "            \n",
    "    #             ecp_pp[round][0][ref_i][key], alpha_pp[round][0][ref_i][key], ecp_zz[round][0][ref_i][key], alpha_zz[round][0][ref_i][key], f_pp[round][0][ref_i][key], f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "    #                 draws1d[round][key],\n",
    "    #                 samples['params'][:,[POIs[i]]],\n",
    "    #                 weights = weights1d[round][key],\n",
    "    #                 theta_names=A.param_names[POIs[i]],\n",
    "    #                 bounds = np.array(bounds)[[POIs[i]]],\n",
    "    #                 references = references_2d[ref_i],\n",
    "    #                 device='cuda'\n",
    "    #             )\n",
    "    \n",
    "    #             if ref_i == 0: validation_sums[round][0][key] = 0\n",
    "    #             uncertainty = (interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "    #             validation_sums[round][0][key] += np.sum(((ecp_pp[round][0][ref_i][key]-alpha_pp[round][0][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    \n",
    "    rows = len(POIs)\n",
    "    \n",
    "    for rnd in [3]:\n",
    "        round = 'round_'+str(rnd)\n",
    "        for ref_i in range(len(references_2d)): \n",
    "            row = 0\n",
    "            column = 0\n",
    "            for i, key in enumerate(draws2d[round].keys()):\n",
    "                row+=1\n",
    "                if row >= rows:\n",
    "                    column+=1\n",
    "                    row = 1+column    \n",
    "                ecp_pp[round][1][ref_i][key], alpha_pp[round][1][ref_i][key], ecp_zz[round][1][ref_i][key], alpha_zz[round][1][ref_i][key], f_pp[round][1][ref_i][key],f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "                    draws2d[round][key],\n",
    "                    samples['params'][:,[column,row]],\n",
    "                    weights = weights2d[round][key],\n",
    "                    theta_names=np.array(A.param_names)[[column,row]],\n",
    "                    bounds = np.array(bounds)[[column,row]],\n",
    "                    references = references_2d[ref_i],\n",
    "                    device='cuda'\n",
    "                )\n",
    "    \n",
    "                if ref_i == 0: validation_sums[round][1][key] = 0\n",
    "                uncertainty = (interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "                validation_sums[round][1][key] += np.sum(((ecp_pp[round][1][ref_i][key]-alpha_pp[round][1][ref_i][key])/uncertainty)**2)/n_samps\n",
    "    \n",
    "    # for i, key in enumerate(draws1d['round_0'].keys()):\n",
    "    #     print(key)\n",
    "    #     for rnd in range(which_truncation+1):\n",
    "    #         print(validation_sums['round_'+str(rnd)][0][key])\n",
    "    #     print()\n",
    "    \n",
    "    for i, key in enumerate(draws2d['round_3'].keys()):\n",
    "        print(key)\n",
    "        for rnd in [3]:\n",
    "            print(validation_sums['round_'+str(rnd)][1][key])\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153cb44-e181-4ea7-be03-c5a337b0843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(norm.ppf(0.5+torch.tensor([1.01])/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfcc01d-f07c-433e-8e1f-2a3c66ecfe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_refs = 100\n",
    "references_2d = [\n",
    "    references2D(samples)[0][:,[0]] for _ in range(n_refs)\n",
    "]\n",
    "\n",
    "references_2d = [\n",
    "    references2D(samples)[0] for _ in range(n_refs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffee245-bc1d-4bf8-813a-e14cefc08c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_colors = ['r','y','g','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8424eb1-a563-4485-9503-85470e63d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e075119-036a-4204-ad01-afd58910808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a68c5-f44b-43f9-8892-51a89be958f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc7050-b82c-4110-8ff7-3c6f7b032d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_truncation = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e7ab2-0ce8-4bb7-8e40-2c5eb55cfe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ecp_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "alpha_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "ecp_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "alpha_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "f_pp = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "f_zz = { 'round_'+str(rnd) : [[{} for ref_list in references_2d],[{} for ref_list in references_2d]] for rnd in range(which_truncation+1) }\n",
    "\n",
    "validation_sums = { 'round_'+str(rnd) : [{},{}] for rnd in range(which_truncation+1) }\n",
    "\n",
    "for rnd in range(which_truncation+1):\n",
    "    round = 'round_'+str(rnd)\n",
    "    for ref_i in range(len(references_2d)):\n",
    "\n",
    "        for i, key in enumerate(draws1d[round].keys()):\n",
    "        \n",
    "            ecp_pp[round][0][ref_i][key], alpha_pp[round][0][ref_i][key], ecp_zz[round][0][ref_i][key], alpha_zz[round][0][ref_i][key], f_pp[round][0][ref_i][key], f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "                draws1d[round][key],\n",
    "                samples['params'][:,[POIs[i]]],\n",
    "                weights = weights1d[round][key],\n",
    "                theta_names=A.param_names[POIs[i]],\n",
    "                bounds = np.array(bounds)[[POIs[i]]],\n",
    "                references = references_2d[ref_i],\n",
    "                device='cuda'\n",
    "            )\n",
    "\n",
    "            if ref_i == 0: validation_sums[round][0][key] = 0\n",
    "            uncertainty = (interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][0][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "            validation_sums[round][0][key] += np.sum(((ecp_pp[round][0][ref_i][key]-alpha_pp[round][0][ref_i][key])/uncertainty)**2)/n_samps\n",
    "\n",
    "\n",
    "rows = len(POIs)\n",
    "\n",
    "for rnd in range(which_truncation+1):\n",
    "    round = 'round_'+str(rnd)\n",
    "    for ref_i in range(len(references_2d)): \n",
    "        row = 0\n",
    "        column = 0\n",
    "        for i, key in enumerate(draws2d[round].keys()):\n",
    "            row+=1\n",
    "            if row >= rows:\n",
    "                column+=1\n",
    "                row = 1+column    \n",
    "            ecp_pp[round][1][ref_i][key], alpha_pp[round][1][ref_i][key], ecp_zz[round][1][ref_i][key], alpha_zz[round][1][ref_i][key], f_pp[round][1][ref_i][key],f_zz[round][0][ref_i][key], _ = get_drp_coverage_torch(\n",
    "                draws2d[round][key],\n",
    "                samples['params'][:,[column,row]],\n",
    "                weights = weights2d[round][key],\n",
    "                theta_names=np.array(A.param_names)[[column,row]],\n",
    "                bounds = np.array(bounds)[[column,row]],\n",
    "                references = references_2d[ref_i],\n",
    "                device='cuda'\n",
    "            )\n",
    "\n",
    "            if ref_i == 0: validation_sums[round][1][key] = 0\n",
    "            uncertainty = (interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,1]-interval((alpha_pp[round][1][ref_i][key]*n_samps).astype(int),n_samps)[:,0])/2\n",
    "            validation_sums[round][1][key] += np.sum(((ecp_pp[round][1][ref_i][key]-alpha_pp[round][1][ref_i][key])/uncertainty)**2)/n_samps\n",
    "\n",
    "for i, key in enumerate(draws1d['round_0'].keys()):\n",
    "    print(key)\n",
    "    for rnd in range(which_truncation+1):\n",
    "        print(validation_sums['round_'+str(rnd)][0][key])\n",
    "    print()\n",
    "\n",
    "for i, key in enumerate(draws2d['round_0'].keys()):\n",
    "    print(key)\n",
    "    for rnd in range(which_truncation+1):\n",
    "        print(validation_sums['round_'+str(rnd)][1][key])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b308b-5258-423c-90c8-c3361b767e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff8615-b9db-47af-8960-30c00fc1d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ecp_pp['round_3'][0][0]['m']\n",
    "a = alpha_pp['round_3'][0][0]['m']\n",
    "np.sum((e-a)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387e7d1-50cc-4664-ae9f-0538d4f08aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8221b2-02f7-4298-9e52-ef67e900eaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc8844-083d-4dce-b731-82e8d2125d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02609d0d-b8c3-4008-aaea-48865a34fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(f_zz['round_3'][0][0]['m'], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e74571-219a-4416-a305-c876e5effe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_to_z(x):\n",
    "    return stdnorm.ppf(0.5+x/2)\n",
    "\n",
    "def z_to_p(x):\n",
    "    return stdnorm.cdf(x)-stdnorm.cdf(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3902a1-2e1a-4bf0-b07f-7cdb61c94903",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_z =3\n",
    "DRP_fig_pp = plt.figure(figsize = (12, 12))\n",
    "DRP_fig_zz = plt.figure(figsize = (12, 12))\n",
    "for i, key in enumerate(draws1d['round_0'].keys()):\n",
    "    # plt.xlabel(\"Credibility level (alpha)\")\n",
    "    # plt.ylabel(\"ECP\")\n",
    "    DRP_fig_pp.add_subplot(rows, rows, i+1+i*rows)\n",
    "    DRP_fig_zz.add_subplot(rows, rows, i+1+i*rows)\n",
    "    # DRP_fig_zz.axes[-1].set_xlim([0,max_z])\n",
    "    # DRP_fig_zz.axes[-1].set_ylim([0,max_z])\n",
    "    for rnd in range(which_truncation+1):\n",
    "        round = 'round_'+str(rnd)\n",
    "        for ref_i in range(len(references_2d)):\n",
    "            ecp_ex_pp = np.zeros(len(ecp_pp[round][0][ref_i][key])+1)\n",
    "            alpha_ex_pp = np.zeros(len(alpha_pp[round][0][ref_i][key])+1)\n",
    "            ecp_ex_zz = np.zeros(len(ecp_zz[round][0][ref_i][key])+1)\n",
    "            alpha_ex_zz = np.zeros(len(alpha_zz[round][0][ref_i][key])+1)\n",
    "            ecp_ex_pp[1:] = ecp_pp[round][0][ref_i][key]\n",
    "            alpha_ex_pp[1:] = alpha_pp[round][0][ref_i][key]\n",
    "            ecp_ex_zz[1:] = ecp_zz[round][0][ref_i][key]\n",
    "            alpha_ex_zz[1:] = alpha_zz[round][0][ref_i][key]\n",
    "            DRP_fig_pp.axes[-1].plot(alpha_ex_pp, ecp_ex_pp, round_colors[rnd],alpha=1)\n",
    "            DRP_fig_zz.axes[-1].plot(alpha_ex_zz, ecp_ex_zz, round_colors[rnd],alpha=1)\n",
    "            \n",
    "        DRP_fig_pp.axes[-1].plot([0,1],[0,1], 'k--')\n",
    "        DRP_fig_pp.axes[-1].plot(alpha_ex_pp,interval((alpha_ex_pp*n_samps).astype(int),n_samps)[:,0], 'k:')\n",
    "        DRP_fig_pp.axes[-1].plot(alpha_ex_pp,interval((alpha_ex_pp*n_samps).astype(int),n_samps)[:,1], 'k:')\n",
    "        DRP_fig_zz.axes[-1].plot([0,max_z],[0,max_z], 'k--')\n",
    "        DRP_fig_zz.axes[-1].plot(alpha_ex_zz,p_to_z(interval((z_to_p(alpha_ex_zz)*n_samps).astype(int),n_samps)[:,0]), 'k:')\n",
    "        DRP_fig_zz.axes[-1].plot(alpha_ex_zz,p_to_z(interval((z_to_p(alpha_ex_zz)*n_samps).astype(int),n_samps)[:,1]), 'k:')\n",
    "\n",
    "\n",
    "row = 0\n",
    "column = 0\n",
    "for i, key in enumerate(draws2d['round_0'].keys()):\n",
    "    row+=1\n",
    "    if row >= rows:\n",
    "        column+=1\n",
    "        row = 1+column \n",
    "    # plt.xlabel(\"Credibility level\")\n",
    "    # plt.ylabel(\"ECP\")\n",
    "    DRP_fig_pp.add_subplot(rows, rows, rows*row+column+1)\n",
    "    DRP_fig_zz.add_subplot(rows, rows, rows*row+column+1)\n",
    "    # DRP_fig_zz.axes[-1].set_xlim([0,max_z])\n",
    "    # DRP_fig_zz.axes[-1].set_ylim([0,max_z])\n",
    "    for rnd in range(which_truncation+1):\n",
    "        round = 'round_'+str(rnd)\n",
    "        for ref_i in range(len(references_2d)):\n",
    "            ecp_ex_pp = np.zeros(len(ecp_pp[round][1][ref_i][key])+1)\n",
    "            alpha_ex_pp = np.zeros(len(alpha_pp[round][1][ref_i][key])+1)\n",
    "            ecp_ex_zz = np.zeros(len(ecp_zz[round][1][ref_i][key])+1)\n",
    "            alpha_ex_zz = np.zeros(len(alpha_zz[round][1][ref_i][key])+1)\n",
    "            ecp_ex_pp[1:] = ecp_pp[round][1][ref_i][key]\n",
    "            alpha_ex_pp[1:] = alpha_pp[round][1][ref_i][key]\n",
    "            ecp_ex_zz[1:] = ecp_zz[round][1][ref_i][key]\n",
    "            alpha_ex_zz[1:] = alpha_zz[round][1][ref_i][key]\n",
    "            DRP_fig_pp.axes[-1].plot(alpha_ex_pp, ecp_ex_pp, round_colors[rnd],alpha=1)\n",
    "            DRP_fig_zz.axes[-1].plot(alpha_ex_zz, ecp_ex_zz, round_colors[rnd],alpha=1)\n",
    "            \n",
    "        DRP_fig_pp.axes[-1].plot([0,1],[0,1], 'k--')\n",
    "        DRP_fig_pp.axes[-1].plot(alpha_ex,interval((alpha_ex*n_samps).astype(int),n_samps)[:,0], 'k:')\n",
    "        DRP_fig_pp.axes[-1].plot(alpha_ex,interval((alpha_ex*n_samps).astype(int),n_samps)[:,1], 'k:')\n",
    "        DRP_fig_zz.axes[-1].plot([0,max_z],[0,max_z], 'k--')\n",
    "        DRP_fig_zz.axes[-1].plot(alpha_ex_zz,p_to_z(interval((z_to_p(alpha_ex_zz)*n_samps).astype(int),n_samps)[:,0]), 'k:')\n",
    "        DRP_fig_zz.axes[-1].plot(alpha_ex_zz,p_to_z(interval((z_to_p(alpha_ex_zz)*n_samps).astype(int),n_samps)[:,1]), 'k:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f4742-9bfd-4b26-86ed-ea797f3ee575",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ax in DRP_fig_pp.axes:\n",
    "    ax.set_visible(False)\n",
    "DRP_fig_pp.axes[5].set_visible(True)\n",
    "DRP_fig_pp.set_figheight(12*5)\n",
    "DRP_fig_pp.set_figwidth(12*5)\n",
    "DRP_fig_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f2b1b-9310-414e-a5bf-84c0aa845d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ax in DRP_fig_zz.axes:\n",
    "    ax.set_visible(False)\n",
    "DRP_fig_zz.axes[5].set_visible(True)\n",
    "DRP_fig_zz.set_figheight(12*5)\n",
    "DRP_fig_zz.set_figwidth(12*5)\n",
    "DRP_fig_zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5717eb-a892-4abf-8c41-c61a589b5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(size=(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e0ba5-32d3-4023-9514-dbf6e2d29d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009aedb4-4f84-4a88-a323-c8ffa9633a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([torch.tensor([2,3,4]),torch.tensor([2,3,4])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe7d0275-90e8-4ff6-a5fc-1794d9dfca50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "autodetected range of [nan, nan] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/swyft4-dev-notebook/lib/python3.9/site-packages/numpy/lib/histograms.py:792\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03mCompute the histogram of a set of data.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m \n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    790\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 792\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/swyft4-dev-notebook/lib/python3.9/site-packages/numpy/lib/histograms.py:426\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_equal_bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[43m_get_outer_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(bins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    429\u001b[0m     bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(bins)\n",
      "File \u001b[0;32m~/.conda/envs/swyft4-dev-notebook/lib/python3.9/site-packages/numpy/lib/histograms.py:323\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[0;34m(a, range)\u001b[0m\n\u001b[1;32m    321\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mmin(), a\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(first_edge) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(last_edge)):\n\u001b[0;32m--> 323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautodetected range of [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] is not finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(first_edge, last_edge))\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# expand empty range to avoid divide by zero\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_edge \u001b[38;5;241m==\u001b[39m last_edge:\n",
      "\u001b[0;31mValueError\u001b[0m: autodetected range of [nan, nan] is not finite"
     ]
    }
   ],
   "source": [
    "np.histogram(np.array([np.nan]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
