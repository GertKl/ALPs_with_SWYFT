{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d9ec14a-ee5b-4572-ba67-4a239f003571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import swyft\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import importlib\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "device_notebook = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import wandb\n",
    "import copy\n",
    "from torch.multiprocessing import Pool\n",
    "torch.multiprocessing.set_start_method('spawn',force=True)\n",
    "torch.set_num_threads(28)\n",
    "import itertools\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b0e9ad-00f5-4c42-8674-004bd3fe210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/home/gertwk/ALPs_with_SWYFT/analysis_scripts/ALP_sim')\n",
    "# from ALP_quick_sim import ALP_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75fd7ce4-6c3c-40dd-8888-7e153efd0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['agnostic3','confident2']\n",
    "colors_priors = ['r','#FFA500','y','g','b', ]\n",
    "\n",
    "priors = {}\n",
    "for ip, name in enumerate(names):\n",
    "\n",
    "    priors[name] = {'name': name}\n",
    "\n",
    "    sys.path.append('/home/gertwk/ALPs_with_SWYFT/cluster_runs/analysis_results/'+name)\n",
    "    import param_function\n",
    "    import ALP_quick_sim\n",
    "    \n",
    "    priors[name]['config_phys'] = '/home/gertwk/ALPs_with_SWYFT/cluster_runs/analysis_results/'+name+'/physics_variables.pickle'\n",
    "    priors[name]['truncation_record'] = '/home/gertwk/ALPs_with_SWYFT/cluster_runs/analysis_results/'+name+'/truncation_record.pickle'\n",
    "\n",
    "    with open(priors[name]['config_phys'], 'rb') as file: config_objects = pickle.load(file)\n",
    "    for key in config_objects.keys(): priors[name][key] = config_objects[key]\n",
    "\n",
    "    with open(priors[name]['truncation_record'], 'rb') as file: config_objects = pickle.load(file)\n",
    "    for key in config_objects.keys(): priors[name][key] = config_objects[key]\n",
    "    \n",
    "    priors[name]['net_path'] = ('/home/gertwk/ALPs_with_SWYFT/cluster_runs/analysis_results/'+priors[name]['name']+\n",
    "                                '/train_output/net/trained_network_round_'+str(priors[name]['which_truncation'])+\n",
    "                                '_gridpoint_'+str(priors[name]['which_grid_point'])+'.pt')\n",
    "\n",
    "    \n",
    "\n",
    "    sys.path.remove('/home/gertwk/ALPs_with_SWYFT/cluster_runs/analysis_results/'+name)\n",
    "    del sys.modules['param_function']\n",
    "    del sys.modules['ALP_quick_sim']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d500d-92e1-496d-9c31-20b43fd5e32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd65f07-af57-4861-961a-33d1ec3c2c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581eb462-4032-439e-86a5-0fc272d813fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_exp_priors = plt.subplots(1,1)[0]\n",
    "\n",
    "for i, prior in enumerate(priors):\n",
    "\n",
    "    with open(prior['config_phys'], 'rb') as file:\n",
    "        config_objects = pickle.load(file)\n",
    "        for key in config_objects.keys()\n",
    "        locals()[key+prior['name']] = config_objects[key]\n",
    "    with open(prior['truncation_record'], 'rb') as file:\n",
    "        config_objects = pickle.load(file)\n",
    "        locals()[key+prior['name']] = config_objects[key]\n",
    "\n",
    "    count = 0\n",
    "    for combo in itertools.product(*hyperparams.values()):\n",
    "        if count == grid_point:\n",
    "            hyperparams_point = {}\n",
    "            print(\"Model number \" + str(count))\n",
    "            for i, key in enumerate(hyperparams.keys()):\n",
    "                hyperparams_point[key]=combo[i]\n",
    "        count +=1\n",
    "    hyperparams_point\n",
    "\n",
    "    \n",
    "    pname = prior['name']\n",
    "    \n",
    "    nets[pname] = NetworkCorner(nbins=90, marginals=[0,1,2,3,4], param_names=A.param_names, **hyperparams_point)\n",
    "    nets[pname].load_state_dict(torch.load(prior['net_path']))\n",
    "\n",
    "    with open(prior['config_phys'], 'rb') as file:\n",
    "        config_objects = pickle.load(file)\n",
    "        locals()['prior_funcs_'+prior['name']] = config_objects['prior_funcs']\n",
    "    with open(prior['truncation_record'], 'rb') as file:\n",
    "        config_objects = pickle.load(file)\n",
    "        locals()['bounds_'+prior['name']] = config_objects['bounds_rounds'][prior['which_gridpoint']][prior['which_truncation']]\n",
    "        \n",
    "    \n",
    "    prior_samps[pname] = ALP_SWYFT_Simulator(A,\n",
    "                                             eval('bounds_'+pname), \n",
    "                                             prior_funcs=eval('prior_funcs_'+pname),\n",
    "                                            ).sample(400_000,targets=['params'])\n",
    "\n",
    "    matrices[pname], _ = generate_expected_limits(nets[pname],averaging_samples[:1000], prior_samps[pname], bounds = [bounds[0], bounds[1]],\n",
    "                        colors = [colors_priors[i]]*5,alpha_variable=True, ax = fig_exp_priors.axes[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d023e335-3246-4472-9984-7ea2635cd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_HDI_thresholds(x, cred_level=[0.68268, 0.95450, 0.99730]):\n",
    "    x = x.flatten()\n",
    "    x = np.sort(x)[::-1]  # Sort backwards\n",
    "    total_mass = x.sum()\n",
    "    enclosed_mass = np.cumsum(x)\n",
    "    idx = [np.argmax(enclosed_mass >= total_mass * f) for f in cred_level]\n",
    "    levels = np.array(x[idx])\n",
    "    return levels\n",
    "    \n",
    "def generate_expected_limits(net,\n",
    "                             samples,\n",
    "                             prior_samples,\n",
    "                             bounds,\n",
    "                             contour_matrix = None,\n",
    "                             predictions = None,\n",
    "                             ax=None,\n",
    "                             limit_credibility=0.9973,\n",
    "                             levels = [0.003,0.05,0.34,0.682,0.95,0.9973,1],\n",
    "                             fill=True,\n",
    "                             bins=50,\n",
    "                             batch_size = 1024,\n",
    "                             param_names = ['m','g'],\n",
    "                             colors = ['r','#FFA500','y','g','b','k'],\n",
    "                             alpha = 0.5,\n",
    "                             alpha_variable = False,\n",
    "                            ):\n",
    "\n",
    "    n_limits = len(samples)\n",
    "    n_prior_samples = len(prior_samples)\n",
    "    \n",
    "    if not np.any(predictions) and not np.any(contour_matrix):\n",
    "        repeat = n_prior_samples // batch_size + (n_prior_samples % batch_size > 0)\n",
    "        \n",
    "        predictions = trainer.infer(\n",
    "            net,\n",
    "            samples.get_dataloader(batch_size=1,repeat=repeat),\n",
    "            prior_samples.get_dataloader(batch_size=batch_size)\n",
    "        )\n",
    "        \n",
    "    if not np.any(contour_matrix):\n",
    "        for i in range(n_limits):\n",
    "    \n",
    "            predictions_i = copy.deepcopy(predictions)\n",
    "    \n",
    "            predictions_i[0].logratios = predictions[0].logratios[i*n_prior_samples:(i+1)*n_prior_samples]\n",
    "            predictions_i[0].params = predictions[0].params[i*n_prior_samples:(i+1)*n_prior_samples]\n",
    "            predictions_i[1].logratios = predictions[1].logratios[i*n_prior_samples:(i+1)*n_prior_samples]\n",
    "            predictions_i[1].params = predictions[1].params[i*n_prior_samples:(i+1)*n_prior_samples]\n",
    "    \n",
    "            counts, _ = swyft.get_pdf(\n",
    "                predictions_i,\n",
    "                param_names,\n",
    "                bins = bins,\n",
    "            )\n",
    "    \n",
    "            if i==0:\n",
    "                X,Y = np.meshgrid(np.linspace(0,counts.shape[0]-1,counts.shape[0]),np.linspace(0,counts.shape[1]-1,counts.shape[1]))\n",
    "                matrix_total = np.zeros(X.shape)\n",
    "    \n",
    "            plt.figure('dummy')\n",
    "            levels_limits=sorted(_get_HDI_thresholds(counts,cred_level=[0,limit_credibility]))\n",
    "            limit_contour = plt.contourf(counts.T,levels=levels_limits)\n",
    "            # plt.clf()\n",
    "    \n",
    "            matrix_i = np.ones(X.shape)\n",
    "    \n",
    "            for collection in limit_contour.collections:\n",
    "                for path in collection.get_paths():\n",
    "                    mask = path.contains_points(np.vstack((X.flatten(), Y.flatten())).T,radius=1e-9)\n",
    "                    mask = mask.reshape(X.shape)\n",
    "                    matrix_i[mask] = 0\n",
    "            \n",
    "            matrix_total += matrix_i\n",
    "    \n",
    "    else:\n",
    "        matrix_total = contour_matrix\n",
    "\n",
    "    if not ax:\n",
    "        fig = plt.figure()\n",
    "        fig.add_subplot(1,1,1)\n",
    "        ax = fig.axes[0]\n",
    "    \n",
    "    for li in range(len(levels)-1):\n",
    "        ax.contourf(matrix_total,\n",
    "                    levels=[levels[li]*n_limits,levels[li+1]*n_limits],\n",
    "                    extent=[bounds[0][0], bounds[0][1], bounds[1][0], bounds[1][1]],\n",
    "                    colors = colors[li],\n",
    "                    alpha = (li+2)*alpha/len(levels) if alpha_variable else alpha,\n",
    "                   )\n",
    "\n",
    "    plt.close('dummy')\n",
    "    \n",
    "    if not ax:\n",
    "        return matrix_total, fig\n",
    "    else:\n",
    "        return matrix_total,ax\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
